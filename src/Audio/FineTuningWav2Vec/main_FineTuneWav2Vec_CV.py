"""
    Main script to fine-tuning the Wav2Vec model.

	author: Cristina Luna. Adapted from the tutorial: https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb
	date: 03/2022


	Usage:
		e.g.
		 python3 MMEmotionRecognition/src/Audio/FineTuningWav2Vec/main_FineTuneWav2Vec_CV.py
		 --audios_dir <RAVDESS_dir>/audios_16kHz
		 --cache_dir MMEmotionRecognition/data/Audio/cache_dir
		 --out_dir <RAVDESS_dir>/FineTuningWav2Vec2_out
         --model_id jonatasgrosman/wav2vec2-large-xlsr-53-english
	Options:
        --audios_dir Path to the directory with the audios to train the model
		--cache_dir Path to the folder to save auxiliar data such as transformer models
		--out_dir: Path to save the trained models, datasets and logs
        --model_id: Name of the model from the Hugging Face repository to use as baseline. In our case: 'jonatasgrosman/wav2vec2-large-xlsr-53-english'
"""

import os
import sys
import argparse
sys.path.append('.')
sys.path.append('..')
sys.path.append('../../')
sys.path.append('../../../')

os.environ['LC_ALL'] ='C.UTF-8'
os.environ['LANG'] = 'C.UTF-8'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
import random
import numpy as np
import pandas as pd
import time

from pathlib import Path
from tqdm import tqdm

import torchaudio

#PREPARE DATA
from datasets import load_dataset, load_metric
#LOAD PRE-TRAINED MODEL ON ASR
from transformers import AutoConfig

from src.Audio.FineTuningWav2Vec.DataCollatorCTCWithPadding import *
from src.Audio.FineTuningWav2Vec.Wav2VecAuxClasses import *
from src.Audio.FineTuningWav2Vec.CTCTrainer import *

#TRAINING
from transformers import EvalPrediction
from transformers import TrainingArguments
from datetime import datetime

from datasets import set_caching_enabled
set_caching_enabled(False)

def seed_libs(seed=2020):
    """
       Fix the seeds for the random generators of torch and other libraries
       :param seed: Seed to pass to the random seed generators
       """
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)


def seed_torch(seed=2020):
    """
    Fix the seeds for the random generators of torch and other libraries
    :param seed: Seed to pass to the random seed generators
    """

    seed_libs(2020)
    os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"

    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)

def prepare_RAVDESS_DS(path_audios):
    """
    Generation of the dataframe with the information of the dataset. The dataframe has the following structure:
     ______________________________________________________________________________________________________________________________
    |             name            |                     path                                   |     emotion      |     actor     |
    ______________________________________________________________________________________________________________________________
    |  01-01-01-01-01-01-01.wav   |    <RAVDESS_dir>/audios_16kHz/01-01-01-01-01-01-01.wav     |     Neutral      |     1         |
    ______________________________________________________________________________________________________________________________
    ...

    :param path_audios: Path to the folder that contains all the audios in .wav format, 16kHz and single-channel(mono)
    """
    dict_emotions_ravdess = {
        0: 'Neutral',
        1: 'Calm',
        2: 'Happy',
        3: 'Sad',
        4: 'Angry',
        5: 'Fear',
        6: 'Disgust',
        7: 'Surprise'
    }
    data = []
    for path in tqdm(Path(path_audios).glob("**/*.wav")):
        name = str(path).split('/')[-1].split('.')[0]
        label = dict_emotions_ravdess[int(name.split("-")[2]) - 1]  # Start emotions in 0
        actor = int(name.split("-")[-1])

        try:
            data.append({
                "name": name,
                "path": path,
                "emotion": label,
                "actor": actor
            })
        except Exception as e:
            # print(str(path), e)
            pass
    df = pd.DataFrame(data)
    return df



def generate_train_test(fold, df, save_path=""):
    """
    Divide the data in train and test in a subject-wise 5-CV way. The division is generated before running the training
    of each fold.

    :param fold:[int] Fold to create the train and test sets [ranging from 0 - 4]
    :param df:[DataFrame] Dataframe with the complete list of files generated by prepare_RAVDESS_DS(..) function
    :param save_path:[str] Path to save the train.csv and test.csv per fold
    """
    actors_per_fold = {
        0: [2,5,14,15,16],
        1: [3, 6, 7, 13, 18],
        2: [10, 11, 12, 19, 20],
        3: [8, 17, 21, 23, 24],
        4: [1, 4, 9, 22],
    }

    test_df = df.loc[df['actor'].isin(actors_per_fold[fold])]
    train_df = df.loc[~df['actor'].isin(actors_per_fold[fold])]

    train_df = train_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)

    if(save_path!=""):
        train_df.to_csv(f"{save_path}/train.csv", sep="\t", encoding="utf-8", index=False)
        test_df.to_csv(f"{save_path}/test.csv", sep="\t", encoding="utf-8", index=False)
    return train_df, test_df

def speech_file_to_array_fn(path):
    """
    Loader of audio recordings. It loads the recordings and convert them to a specific sampling rate if required, and returns
    an array with the samples of the audio.

    :param path:[str] Path to the wav file.
    :param target_sampling_rate:[int] Global variable with the expected sampling rate of the model
    """
    speech_array, sampling_rate = torchaudio.load(path)
    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)
    speech = resampler(speech_array).squeeze().numpy()
    return speech

def label_to_id(label, label_list):

    if len(label_list) > 0:
        return label_list.index(label) if label in label_list else -1

    return label

def preprocess_function(examples, input_column = "path", output_column = "emotion"):
    """
    Load the recordings with their labels.

    :param examples:[DataFrame]  with the samples of the training or test sets.
    :param input_column:[str]  Column that contain the paths to the recordings
    :param output_column:[str]  Column that contain the emotion associated to each recording
    :param target_sampling_rate:[int] Global variable with the expected sampling rate of the model
    """
    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]
    target_list = [label_to_id(label, label_list) for label in examples[output_column]]

    result = processor(speech_list, sampling_rate=target_sampling_rate)
    result["labels"] = list(target_list)

    return result

def compute_metrics(p: EvalPrediction):
    """
    Extract the metrics of the model from the predictions.
        -MSE for regression tasks
        -Accuracy for classification tasks
    :param p: EvalPrediction: Predictions of the model.
   """
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)

    if is_regression:
        return {"mse": ((preds - p.label_ids) ** 2).mean().item()}
    else:
        return {"accuracy": (preds == p.label_ids).astype(np.float32).mean().item()}


if __name__ == '__main__':
    #Read input parameters
    parser = argparse.ArgumentParser(description="Configuration of setup and training process")
    parser.add_argument('-audios', '--audios_dir', type=str, required=True,
                        help='Path with the audios to train/test the model')
    parser.add_argument('-cache', '--cache_dir', type=str, required=True,
                        help='Path to save aux. data in cache')
    parser.add_argument('-out', '--out_dir', type=str, help='Path to save the embeddings extracted from the model',
                        default='./')
    parser.add_argument('-model', '--model_id', type=str,
                        help='Model identificator in Hugging Face library [default: jonatasgrosman/wav2vec2-large-xlsr-53-english]',
                        default='jonatasgrosman/wav2vec2-large-xlsr-53-english')

    args = parser.parse_args()
    #PARAMETERS #######################
    out_dir_models = os.path.join(args.out_dir, "trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition") #out path to save trained models
    data_path = os.path.join(args.out_dir,"data") #Path to save csvs generated containing the recording information (path, name, emotion...)
    # We need to specify the input and output column
    input_column = "path" # Name of the column that will contain the path of the recordings
    output_column = "emotion" # Name of the column that will contain the labels of the recordings
    pooling_mode = "mean" #Type of pooling to apply to the embeddings generated ath the output of the transformer module to collapse all the timesteps of the recordingsinto a single vector
    now = datetime.now()
    current_time = now.strftime("%Y%m%d_%H%M%S")
    seed = 2020
    epochs = 10 #Epochs to train the model
    #PARAMETERS #######################
    seed_torch(seed=seed) #Set random seeds



    for fold in range(5): # 5-CV strategy
        #Define paths, create aux. folders and callbacks to save data
        out_dir_models_path = os.path.join(out_dir_models, current_time, "fold"+str(fold))
        save_path = os.path.join(data_path, current_time, "fold"+str(fold))
        os.environ['TRANSFORMERS_CACHE'] = os.path.join(args.cache_dir, current_time, "fold"+str(fold))
        os.environ['HF_DATASETS_CACHE'] = os.path.join(args.cache_dir, current_time, "fold"+str(fold))
        os.makedirs(save_path, exist_ok=True)
        print("SAVING DATA IN: ", save_path)
        callbackTB = transformers.integrations.TensorBoardCallback()
        #######################

        #PREPARE DATASET
        #Generate complete dataframe with RAVDESS samples
        df = prepare_RAVDESS_DS(args.audios_dir)
        _, _ = generate_train_test(fold, df, save_path)
        time.sleep(10) #wait some time to get the dataset ready


        data_files = {
            "train": os.path.join(save_path, "train.csv"),
            "validation": os.path.join(save_path, "test.csv"),
        }
        #Load data
        dataset = load_dataset("csv", data_files=data_files, delimiter="\t", )
        train_dataset = dataset["train"]
        eval_dataset = dataset["validation"]
        print("Processing fold: ", str(fold), " - actors in Train fold: ",set(train_dataset["actor"]))
        print("Processing fold: ", str(fold), " - actors in Eval fold: ", set(eval_dataset["actor"]))
        label_list = train_dataset.unique(output_column)
        label_list.sort()  # Let's sort it for determinism
        num_labels = len(label_list)
        print(f"A classification problem with {num_labels} classes: {label_list}")
        # LOAD PRE-TRAINED MODEL ON ASR
        # config
        config = AutoConfig.from_pretrained(
            args.model_id, #path to the model of HuggingFace lib. that we will use as baseline to fine-tune.
            num_labels=num_labels, # num classes
            label2id={label: i for i, label in enumerate(label_list)}, # dict that maps emotions -> numbers
            id2label={i: label for i, label in enumerate(label_list)}, # dict that maps numbers -> emotions
            finetuning_task="wav2vec2_clf",
        )
        #Add in the config variable the 'pooling_mode'
        setattr(config, 'pooling_mode', pooling_mode)
        #Load the processor for the type of model (Wav2Vec2.0 in our case) and get the expected sampling rate (16kHZ in our case)
        processor = Wav2Vec2Processor.from_pretrained(args.model_id, )
        target_sampling_rate = processor.feature_extractor.sampling_rate
        print(f"The target sampling rate: {target_sampling_rate}")

        print("Generating training...")
        train_dataset = train_dataset.map(
            preprocess_function,
            batch_size=100,
            batched=True,
            num_proc=4
        )
        print("Generating test...")
        eval_dataset = eval_dataset.map(
            preprocess_function,
            batch_size=100,
            batched=True,
            num_proc=4
        )
        #MODEL
        print("Training model...")
        data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)
        is_regression = False

        #Create the architecture: Wav2Vec2.0 model + mean pooling + MLP (1024, 8)
        model = Wav2Vec2ForSpeechClassification.from_pretrained(
            args.model_id,
            config=config,
        )

        #Freeze feature encoder layers (CNNs) of wav2vec2.0 & train the transformer module and the MLP that we have added (randomly initialized)
        model.freeze_feature_extractor()

        #Set trainig arguments/parameters
        training_args = TrainingArguments(
            output_dir=out_dir_models_path,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            gradient_accumulation_steps=2,
            evaluation_strategy="steps",
            prediction_loss_only=False,
            num_train_epochs=epochs,
            fp16=True,
            save_steps=10,
            eval_steps=10,
            logging_steps=10,
            learning_rate=1e-4,
            save_total_limit=5,
            load_best_model_at_end=True,
            metric_for_best_model="eval_accuracy",
            seed=seed, )
        #Set data collator to pad the small recordings
        trainer = CTCTrainer(
            model=model,
            data_collator=data_collator,
            args=training_args,
            compute_metrics=compute_metrics,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=processor.feature_extractor,
            callbacks = [callbackTB])

        #Start training the network using the train_dataset & evaluating it on the eval_dataset passed as parameters
        # to the CTCTrainer
        trainer.train()





